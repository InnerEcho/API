import { InMemoryChatMessageHistory } from '@langchain/core/chat_history';
import { ChatOpenAI } from '@langchain/openai';
import { ChatPromptTemplate } from '@langchain/core/prompts';
import { StringOutputParser } from '@langchain/core/output_parsers';
import { RunnableWithMessageHistory } from '@langchain/core/runnables';
import db from '../../models/index.js';
// Sequelize ëª¨ë¸ì—ì„œ ChatHistory ì¶”ì¶œ
const { ChatHistory } = db;
/**
 * ðŸŒ± BaseChatBot (í…œí”Œë¦¿ ë©”ì„œë“œ íŒ¨í„´ì˜ ì¶”ìƒ í´ëž˜ìŠ¤)
 * - ëŒ€í™” ì²˜ë¦¬ì˜ ê³µí†µ íë¦„ì„ ê³ ì •
 * - í”„ë¡¬í”„íŠ¸ ìƒì„±ì€ í•˜ìœ„ í´ëž˜ìŠ¤ì— ìœ„ìž„
 */
export class BaseChatBot {
    constructor() {
        // ì‚¬ìš©ìžë³„ ëŒ€í™” ì´ë ¥ì„ ë©”ëª¨ë¦¬ì— ì €ìž¥ (ì„¸ì…˜ ê¸°ë°˜)
        this.messageHistories = {};
    }
    /**
     * ðŸŒ± ëŒ€í™” ì²˜ë¦¬ì˜ ê³µí†µ í…œí”Œë¦¿ ë©”ì„œë“œ
     * 1. ì‹ë¬¼ ì •ë³´ ì¡°íšŒ
     * 2. í”„ë¡¬í”„íŠ¸ ìƒì„± (í•˜ìœ„ í´ëž˜ìŠ¤ì— ìœ„ìž„)
     * 3. LLM í˜¸ì¶œ
     * 4. ëŒ€í™” ì´ë ¥ ê´€ë¦¬
     * 5. ëŒ€í™” ê²°ê³¼ ì €ìž¥ ë° ë°˜í™˜
     */
    async processChat(userId, plantId, userMessage) {
        // 1. ì‚¬ìš©ìž & ì‹ë¬¼ ì •ë³´ ì¡°íšŒ
        const plantDbInfoResult = await db.sequelize.query(`
        SELECT u.user_name, p.nickname, s.species_name
        FROM user u, plant p, species s
        WHERE u.user_id = ${userId} AND p.plant_id = ${plantId} AND p.species_id = s.species_id;
      `, { type: db.Sequelize.QueryTypes.SELECT });
        if (!plantDbInfoResult || plantDbInfoResult.length === 0) {
            throw new Error('Not Exists Chatbot DB');
        }
        const plantDbInfo = plantDbInfoResult[0];
        // 2. í”„ë¡¬í”„íŠ¸ ìƒì„± (í•˜ìœ„ í´ëž˜ìŠ¤ì—ì„œ ì •ì˜)
        const prompt = await this.createPrompt(plantDbInfo, userId, plantId, userMessage);
        // 3. LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        const llm = new ChatOpenAI({
            model: 'gpt-4o',
            apiKey: process.env.OPENAI_API_KEY,
        });
        // 4. LLM í˜¸ì¶œ ì²´ì¸ êµ¬ì„±
        const userMessageTemplate = ChatPromptTemplate.fromMessages(prompt);
        const outputParser = new StringOutputParser();
        const llmChain = userMessageTemplate.pipe(llm).pipe(outputParser);
        // 5. ëŒ€í™” ì´ë ¥ ê´€ë¦¬ ì„¤ì •
        const historyChain = new RunnableWithMessageHistory({
            runnable: llmChain,
            getMessageHistory: async (sessionId) => {
                if (!this.messageHistories[sessionId]) {
                    this.messageHistories[sessionId] = new InMemoryChatMessageHistory();
                }
                return this.messageHistories[sessionId];
            },
            inputMessagesKey: 'input',
            historyMessagesKey: 'chat_history',
        });
        // 6. LLM í˜¸ì¶œ ì‹¤í–‰
        const config = { configurable: { sessionId: userId } };
        const botReply = await historyChain.invoke({ input: userMessage }, config);
        return botReply;
    }
}
